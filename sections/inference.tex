\section{Size Inference}\label{sec:algorithm}

In this section, we present a size inference algorithm, based on that of \CIChat~\citep{cic-hat}.
Starting with completely bare terms, corresponding to terms in CIC,
the algorithm assigns size annotations while collecting a set of subsizing constraints.
Because the subsizing constraints that must be satisfied are based on the typing rules,
this algorithm is also necessarily a type checking algorithm,
ensuring well-typedness of the size-annotated term.
Then given a program consisting of bare global declarations,
the algorithm produces sized and well-typed declarations (or fails),
along with a set of constraints on the size variables involved.

Before we add the declarations back into a sized global environment,
we find a solution for the constraints first.
That is, we find an assignment from size variables to size expressions
such that these size expressions satisfy all of the constraints.
Then we perform the substitution of these assignments on the declarations,
and lastly add them into the global environment.
This lets us run the inference algorithm on each declaration independently,
without needing to manipulate a constraint set every time a global declaration is used in a subsequent one.

One of the most involved parts of the algorithm is the size inference and type checking of \cofixpoints.
This uses the \RecCheck algorithm from \Fhat~\citep{f-hat}.
We restate the algorithm in this section for convenience, but not the associated proofs of soundness and completeness.
The other notably involved part of the algorithm is the \solve algorithm,
which given a set of constraints produces a valid solution.
Finally, we state soundness and completeness theorems for the algorithm as a whole,
proving only soundness and leaving completeness as a conjecture.

\subsection{Preliminaries}

We first formally define the notions of constraints and solutions,
as well as some additional notation.

\begin{definition}
A \textbf{subsizing constraint set} (or simply \textbf{constraint set}) $C$ is a set of pairs of size expressions $s_1 \sqsubseteq s_2$ (also referred to as a \textbf{constraint})
representing a subsizing relation from $s_1$ to $s_2$ that must be enforced.
(When ambiguous, we will explicitly distinguish between the \emph{constraint} $s_1 \sqsubseteq s_2$ and the \emph{judgement} $s_1 \sqsubseteq s_2$.)

We write $s_1 = s_2$ to mean the two pairs $s_1 \sqsubseteq s_2$ and $s_2 \sqsubseteq s_1$.
Given a set of size variables $V$, we also write $\upsilon \sqsubseteq V$ for the pointwise constraint set $\set{\upsilon \sqsubseteq \upsilon' \mid \upsilon' \in V}$,
and similarly for $V \sqsubseteq \upsilon$.
\end{definition}

This is the natural representation of the constraints:
the algorithm is based on the typing rules,
and they produce constraints representing subsizing judgements that need to hold.
However, in \RecCheck and in \solve we will need to view these constraints as a graph.
We use $C$ to represent either the constraint set or the graph depending on the context.
First, notice that any noninfinite size consists of a size variable and some concrete number $n$ of successor ``hats'';
we will write this as $\succ{\upsilon}^n$ so that, for instance, $\succ{\succ{\succ{\upsilon}}}$ is instead $\succ{\upsilon}^3$.

\begin{definition}
A \textbf{subsizing constraint graph} (or simply \textbf{constraint graph}) $C$ of a constraint set is a weighted, directed graph whose vertices are size variables, edges are constraints, and weights are integers.

Given a constraint \mbox{$\succ{\upsilon}_1^{n_1} \sqsubseteq \succ{\upsilon}_2^{n_2}$},
the constraint graph contains an edge from $\upsilon_1$ to $\upsilon_2$ with weight $n_2 - n_1$.
Constraints of the form $s \sqsubseteq \infty$ are trivially true and aren't added to the graph.
Constraints of the form $\infty \sqsubseteq \succ{\upsilon}^n$ correspond to an edge from $\infty$ to $\upsilon$ with weight $0$.
\end{definition}

Given a constraint graph and some set of size variables $V$,
it's useful to know which variables in the graph can be reached from $V$ or will reach $V$.

\begin{definition}[Transitive closures]~\\[-4ex]
\begin{itemize}
  \item Given a set of size variables $V$, the \textbf{upward closure} $\bigsqcup V$ with respect to $C$ is the set of variables that can be reached from $V$ by travelling along the directed edges of $C$.
  That is, $V \subseteq \bigsqcup V$, and if $\upsilon_1 \in \bigsqcup V$ and $\succ{\upsilon}_1^{n_1} \sqsubseteq \succ{\upsilon}_2^{n_2}$, then $\upsilon_2 \in \bigsqcup V$.
  \item Given a set of size variables $V$, the \textbf{downward closure} $\bigsqcap V$ with respect to $C$ is the set of variables that can reach $V$ by travelling along the directed edges of $C$.
  That is, $V \subseteq \bigsqcap V$, and if $\upsilon_2 \in \bigsqcap V$ and $\succ{\upsilon}_1^{n_1} \sqsubseteq \succ{\upsilon}_2^{n_2}$, then $\upsilon_1 \in \bigsqcap V$.
\end{itemize}
\end{definition}

Finally, we can define what it means to be a solution of a constraint set,
as well as some useful related notation.

\begin{definition}[Size substitutions and constraint satisfaction]~\\[-4ex]
\begin{itemize}
  \item A \textbf{size substitution} $\rho$ is a map from size variables to size expressions.
  A concrete size substitution is written as $\set{\upsilon_1 \mapsto s_1, \dots, \upsilon_n \mapsto s_n}$.

  \item $\rho$ is implicitly a partial function on size variables, size expressions, and sets of size variables. $\rho(\upsilon)$ returns $s$ if $\upsilon \mapsto s$ is in $\rho$ and $\upsilon$ otherwise. If $s = \succ{\upsilon}^n$, then $\rho(s)$ substitutes $\upsilon$ with $\rho(\upsilon)$; otherwise $\rho(s) = \infty$. Finally, $\rho(V) = \set{\rho(\upsilon) \mid \upsilon \in V}$.

  \item The composition of size substitutions $\rho_1$ and $\rho_2$ is defined as \mbox{$(\rho_1 \circ \rho_2)(\upsilon) \coloneqq \rho_1(\rho_2(\upsilon))$}.

  \item We write $\rho e$ to denote the sized term $e$ with its size variables replaced by their mappings, and similarly for $\rho \Gamma$.

  \item A size substitution $\rho$ \textbf{satisfies the constraint set} $C$ (or is a \textbf{solution} of $C$), written as $\rho \vDash C$, if for every constraint $s_1 \sqsubseteq s_2$ in $C$, the judgement $\rho(s_1) \sqsubseteq \rho(s_2)$ holds.
\end{itemize}
\end{definition}

\input{figures/contexts-gen.tex}

Solutions of constraint sets are used when inferring sizes of global declarations.
The constraint sets are generated when inferring sizes of terms.
Each use of a definition must assign fresh size variables to its body; therefore, we must also propagate the constraints that enforce subsizing relations between those size variables.
To this end, we introduce \emph{generic}%
\footnote{This terminology is borrowed from \citet{universes}, who have similar environments in the context of universe level constraint collection in the presence of cumulativity.}
environments in \autoref{fig:contexts-gen}, where definitions are associated with a set of constraints.
% [This isn't ever used?] As usual, we will use $\Psi$ as shorthand for sized generic environments and $\Psi^\circ$ for bare generic environments.
By abuse of notation, we treat $\Psi(\ph)$ and $\Gamma_G(\ph)$ as lookup functions for $\Psi$ and $\Gamma_G$, where $\Psi(x) = t$ if $(x : t) \in \Psi$ and $\Psi(x) = (C, e : t)$ if $(C, x : t \coloneqq e) \in \Psi$, and similarly for $\Gamma_G$.

We now define three judgements to represent \emph{checking}, \emph{inference}, and \emph{well-formed\-ness}.
They all use the symbol $\rightsquigarrow$, with inputs on the left and outputs on the right.
\begin{itemize}
  \item $\cgp \vdash e^\circ \Leftarrow t \rightsquigarrow C', e$ (checking) takes a set of constraints $C$, environments $\Gamma_G, \Psi$, a bare term $e^\circ$, and an annotated type $t$, and produces the annotated term $e$ with a new set of constraints $C'$ that ensures that the type of $e$ subtypes $t$.
  \item $\cgp \vdash e^\circ \rightsquigarrow C', e \Rightarrow t$ (inference) takes a set of constraints $C$, environments $\Gamma_G, \Psi$, and a bare term $e^\circ$, and produces the annotated term $e$, its annotated type $t$, and a new set of constraints $C'$.
  \item $\Gamma_G^\circ \rightsquigarrow \Gamma_G$ (well-formedness) takes a global environment with bare declarations and produces a global environment where each declaration has been properly annotated via size inference.
\end{itemize}

The algorithm is implicitly parameterized over a fixed signature $\Sigma$,
as well as two mutable sets of size variables $\V, \V^*$, such that $\V^* \subseteq \V$.
Their assignment is denoted with $\coloneqq$ and they are initialized as empty.
The set $\V^*$ contains \textit{position} size variables,
which mark size-preserving types by replacing position annotations,
and we use $\tau$ for these position size variables.
We define two related metafunctions: \PV returns all position size variables in a given term,
while $\erase{\ph}^*$ erases position size variables to position annotations and all other annotations to bare.
Finally, on the right-hand size of inference judgements, we use $e \Rightarrow^* t$ to mean $e \Rightarrow t' \wedge t = \whnf{t'}$.

We define a number of additional metafunctions to translate the side conditions from the typing rules into procedural form.
They are introduced as needed, but are also summarized in \autoref{fig:inference-metafunctions} in \autoref{sec:supplementary}.

\paragraph*{} The entry point of the algorithm is the well-formedness judgement,
whose rules are defined in \autoref{fig:algorithm-wf} and use the mutually-defined rules of the checking and inference judgements,
defined in \autoref{fig:algorithm-check} and \autoref{fig:algorithm} respectively.
We begin with the latter two first in \autoref{subsec:algorithm:infer},
followed by a detailed look at \RecCheck in \autoref{subsec:algorithm:reccheck}.
Well-formedness is discussed in \autoref{subsec:algorithm:wf}.
Finally, we make our way up to soundness and completeness with respect to the typing rules in \autoref{subsec:algorithm:metatheory}.

\subsection{Inference Algorithm}\label{subsec:algorithm:infer}

Size inference begins with either a bare term or a position term. For the bare terms, even type annotations of \cofixpoints are bare, \ie
  $$e^\circ \Coloneqq \dots
    \mid \fix{\seq{n_k}, m}{f}{t^\circ}{e^\circ}
    \mid \cofix{m}{f}{t^\circ}{e^\circ}$$
Notice that fixpoints still have a vector of indices, with $n_k$ being the index of the recursive argument of the $k$th mutual fixpoint, whereas surface Coq programs generally have no indices.
To produce these indices, we do what Coq currently does: brute-force search.
We try the algorithm on every combination of indices from left to right.
This continues until one combination works, or fails if none do.
Then the type annotations are initially position-annotated on as many types as possible,
and this position term itself is passed into the algorithm.
Because of the way the Coq kernel is structured, this may not always be possible in the implementation.
We discuss this and other implementational issues in the next section.

\input{figures/algorithm-check.tex}
\input{figures/algorithm.tex}

\paragraph*{} \refrule{a-check} in \autoref{fig:algorithm-check} is the checking component of the algorithm.
To ensure that the inferred type subtypes the given type, we use the metafunction $\constrain$ that takes two sized terms and attempts to produce a set of subsizing constraints based on the subtyping rules of \autoref{fig:subtyping}.
$\constrain$ may reduce terms to check convertibility and will fail if two terms are incompatible.

\autoref{fig:algorithm} is the inference component of the algorithm. Rules \refnorule{a-var-assum}, \refnorule{a-const-assum}, \refnorule{a-univ}, \refnorule{a-prod}, \refnorule{a-abs}, and \refnorule{a-app} are all fairly straightforward.
Note that after type annotations pass through inference to become sized types, they must be erased to bare types again.
These rules use the metafunctions \axiom, \rules, and \elim, which correspond to the sets \Axioms, \Rules, and \Elims, defined in \autoref{fig:axruel}.
The metafunction \axiom produces the type of a universe; \rules produces the type of a function type given the universes of its argument and return types; and \elim directly checks membership in \Elims and can fail.

\refrule{a-let-in} is also straightforward.
After checking the bound term against the inferred type, we add the generated constraint to the generic local environment before inferring the type of the body.
If size variables appear in the bound term or its type, then the subsizing relations among them are maintained by these constraints.
Note that there may be relations between the size variables in the term and those in the type, which is why, when using variables with definitions, we need to simultaneously replace the size variables in both the term \textit{and} the type.
For instance, consider the following inductive type and bare let expression:
\begin{align*}
  \app{\Box}{(\assm{A}{\Type{}})} : A \to \Type{} &\coloneqq
    \seq{\MkBox : (\assm{a}{A}) \to \app{\Box}{A}{a}} \\
  \letin{b}{\app{\Box}{\Type{}}{\Nat}&}{\app{\MkBox}{\Type{}}{\Nat}}{\dots}
\end{align*}

If we annotate the type as $\app{\Box^{\upsilon_1}}{\Type{}}{\Nat^{\upsilon_2}}$ and size-infer the bound term as $\app{\MkBox}{\Type{}}{\Nat^{\upsilon_3}} : \app{\Box^{\upsilon_4}}{\Type{}}{\Nat^{\upsilon_3}}$, then we will have the constraint set \mbox{$\set{\upsilon_4 \sqsubseteq \upsilon_1, \upsilon_2 = \upsilon_3}$}.

In Rules \refnorule{a-var-def} and \refnorule{a-const-def}, we annotate variables and constants using a vector of annotations from \fresh, which generates the given number of fresh size variables and adds them to $\V$.
The length of the vector corresponds to the number of size annotations found in the body and the type of the definitions.
Reusing the above example, if
$$(\set{\upsilon_4 \sqsubseteq \upsilon_1, \upsilon_2 = \upsilon_3}, \defn{b}{\app{\Box^{\upsilon_1}}{\Type{}}{\Nat^{\upsilon_2}}}{\app{\MkBox}{\Type{}}{\Nat^{\upsilon_3}}}) \in \Psi$$
then a use of $b$ would be annotated with fresh size variables $\upsilon_5, \upsilon_6, \upsilon_7$ as $b^{\seq{\upsilon_5, \upsilon_6, \upsilon_7}}$.
Furthermore, the constraint set we return will include $\set{\upsilon_4 \sqsubseteq \upsilon_5, \upsilon_6 = \upsilon_7}$.
If $b$ is $\delta$\=/reduced during inference, such as in a fixpoint type, then it is replaced by $\app{\MkBox}{\Type{}}{\Nat^{\upsilon_7}}$.

A position term from a position-annotated \cofixpoint type can be passed into the algorithm, so we deal with the possibilities separately in Rules \refnorule{a-ind} and \refnorule{a-ind-star}.
In both rules, a bare \coinductive type is annotated with a size variable; in \refrule{a-ind-star}, it is also added to the set of position size variables $\V^*$.
The position annotation of \coinductive types occurs in \refrule{a-fix} or \refrule{a-cofix}, which we discuss shortly.

In \refrule{a-constr}, we generate a single fresh size variable, which gets annotated on the constructor's \coinductive type in the argument types of the constructor type, as well as the return type, which has the successor of that size variable.
All other \coinductive types which are not the constructor's \coinductive type continue to have $\infty$ annotations.

The key constraint in \refrule{a-case} is generated by \casesize.
Similar to \refrule{a-constr}, we generate a single fresh size variable $\upsilon$ to annotate on $I_k$ in the branches' argument types, which correspond to the constructor arguments of the target.
Then, given the unapplied target type $I_k^s$, \casesize returns $\set{s \sqsubseteq \hat{\upsilon}}$ if $I_k$ is inductive and $\set{\hat{\upsilon} \sqsubseteq s}$ if $I_k$ is coinductive.
This ensures that the target type satisfies $I_k^s ~ \overline{p} ~ \overline{a} \leq I_k^{\hat{\upsilon}_k} ~ \overline{p} ~ \overline{a}$, so that \refrule{case} is satisfied.

The rest of the rule proceeds as we would expect: we infer the sized type of the target and the motive, we check that the motive and the branches have the types we expect given the target type, and we infer that the sized type of the case expression is the annotated motive applied to the target type's annotated indices and the annotated target itself.
We also ensure that the elimination universes are valid using \elim on the motive type's return universe and the target type's universe.
To obtain the motive type's return universe, we use \decompose.
Given a type $t$ and a natural $n$, this metafunction reduces $t$ to a function type $\prodctx{\Delta}{u}$ where $\norm{\Delta} = n$, reduces $u$ to a universe $U$, and returns $U$.
It can fail if $t$ cannot be reduced to a function type, if $\norm{\Delta} < n$, or if $u$ cannot be reduced to a universe.

\paragraph*{} Finally, we come to size inference and termination/productivity checking for \cofixpoints.
It uses the following metafunctions:
\begin{itemize}
  \item \setrecstars, given a function type $t$ and an index $n$, decomposes $t$ into arguments and a return type, reduces the $n$th argument type to an inductive type, annotates that inductive type with position annotation $*$, annotates the return type with $*$ if it has the same inductive type, and rebuilds the function type.
    This is how fixpoint types obtain their position annotations without being user-provided; the algorithm will remove other position annotations if size-preservation fails.
    
    Similarly, \setcorecstars annotates the coinductive return type first, then the argument types with the same coinductive type.
    Both of these can fail if the $n$th argument type or the return type respectively are not \coinductive types.
    Note that the decomposition of $t$ may perform reductions using \whnf.
  \item \getrecvar, given a function type $t$ and an index $n$, returns the position size variable of the annotation on the $n$th inductive argument type, while \getcorecvar returns the position size variable of the annotation on the coinductive return type.
    Essentially, they retrieve the position size variable of the annotation on the primary \corecursive type of a \cofixpoint type.
  \item \shift replaces all position size annotations $s$ (\ie $\floor{s} \in \V^*$) by its successor $\hat{s}$.
\end{itemize}

Although the desired \cofixpoint is the $m$th one in the block of mutually-defined \cofixpoints, we must still size-infer and type-check the entire mutual definition.
Rules \refnorule{a-fix} and \refnorule{a-cofix} first run the size inference algorithm on each of the \cofixpoint \emph{types}, ignoring the results, to ensure that any reduction on those types will terminate.
Then we annotate the bare types with position annotations (using \setrecstars/\setcorecstars) and pass these position types through the algorithm to get sized types $\overline{t_k}$.
Next, we check that the \cofixpoint bodies have the successor-sized types of $\overline{t_k}$ when the \cofixpoints have types $\overline{t_k}$ in the local environment.
Lastly, we call \RecCheckLoop, and return the constraints it gives us, along with the $m$th \cofixpoint type.

\input{figures/helpers.tex}

Notice that \setrecstars and \setcorecstars optimistically annotate \textit{all} possible \coinductive types in the \cofixpoint type with position annotations, but not all \cofixpoints are size-preserving.
\RecCheckLoop filters these annotations to generate the final constraint set.
This is a recursive function that calls \RecCheck, which checks satisfiability of a given constraint set.
If the set is unsatisfiable due to a bad position annotation, then \RecCheckLoop removes the annotation and tries again.

More specifically, \RecCheck can fail raising \RecCheckFail, which contains a set $V$ of position size variables that must be set to infinity; since position size variables always appear on size-preserved types, they cannot be infinite.
\RecCheckLoop then removes $V$ from the set of position size variables, allowing them to be set to infinity, and recursively calls itself.
The number of position size variables from the \cofixpoint type shrinks on every iteration until no more can be removed.
If no satisfiable set is found even when no positions are considered size preserving, termination/productivity checking and thus type inference has failed.
An OCaml-like pseudocode implementation of \RecCheckLoop is provided by \autoref{fig:helpers}.

\subsection{RecCheck}\label{subsec:algorithm:reccheck}

As in previous work on \CChatomega with coinductive streams~\citep{cc-hat-omega} and in \CIChat, we use the same \RecCheck algorithm from \Fhat~\citep{f-hat}.
This algorithm attempts to ensure that the subsizing rules in \autoref{fig:subsizing} can be satisfied within a given set of constraints.
It does so by checking the set of constraints for invalid circular subsizing relations, setting the size variables involved in the cycles to $\infty$, and producing a new set of constraints without these problems; or it fails, indicating nontermination or nonproductivity.
It takes four arguments:

\begin{itemize}
  \item A constraint set $C$, which we treat as a constraint graph.
  \item The size variable $\tau$ of the annotation on the type of the recursive argument (for fixpoints) or on the return type (for cofixpoints). While the return type (for fixpoints) or the types of other arguments (for cofixpoints) may optionally be marked as size-preserving, each \cofixpoint type requires at \textit{least} $\tau$ for the primary \corecursive type.
  \item A set of size variables $V^*$ that must be set to some non-infinite size.
    These are the size annotations in the \cofixpoint type that have position size variables.
    Note that $\tau \in V^*$.
  \item A set of size variables $V^\neq$ that must be set to $\infty$.
    These are all other non-position size annotations, found in the \cofixpoint types and bodies.
\end{itemize}

The key idea of the algorithm is that if there is a negative cycle in $C$,
then for any size variable $\upsilon$ in the cycle,
supposing that the total weight going once around the cycle is $-n$,
by transitivity we have the subsizing relation $\succ{\upsilon}^{n} \sqsubseteq \upsilon$,
This relation can only hold if $\upsilon = \infty$,
since $\succ{\infty} \sqsubseteq \infty$.
The algorithm proceeds as follows:

\begin{enumerate}
  \item \label{alg:reccheck:smallest} Let $V^\iota = \bigsqcap V^*$, and add $\tau \sqsubseteq V^\iota$ to $C$.
    This ensures that $\tau$ is the smallest size variable among all the noninfinite size variables.
  \item \label{alg:reccheck:neg-cycles} Find all negative cycles in $C$, and let $V^-$ be the set of all size variables in some negative cycle.
  \item Remove all edges with size variables in $V^-$ from $C$, and add $\infty \sqsubseteq V^-$.
  \item Add $\infty \sqsubseteq \left(\bigsqcup V^\neq \cap \bigsqcup V^\iota\right)$ to $C$.
  \item Let $V^\bot = \left(\bigsqcup \set{\infty}\right) \cap V^\iota$.
    This is the set of size variables that we have determined to both be infinite and noninfinite.
    If $V^\bot$ is empty, then return $C$.
  \item Otherwise, let $V = V^\bot \cap (V^* \setminus \set{\tau})$, and fail with $\RecCheckFail{V}$.
    This is the set of contradictory position size variables excluding $\tau$, which we can remove from $\V^*$ in \RecCheckLoop.
    If $V$ is empty, there are no position size variables left to remove, so the check and therefore the size inference algorithm fails.
\end{enumerate}

Disregarding closure operations and set operations like intersection and difference, the time complexity of a single pass is $O(\norm{V}\norm{C})$, where $V$ is the set of size variables appearing in $C$.
This comes from the negative-cycle finding in (\ref{alg:reccheck:neg-cycles}) using, for instance, the Bellman--Ford algorithm.

\input{figures/algorithm-wf.tex}

\subsection{Well-Formedness}\label{subsec:algorithm:wf}

A self-contained chunk of code, be it a file or a module, consists of a sequence of \coinductive definitions (signatures) and programs (global declarations).
For our purposes, we assume that there is a singular well-formed signature defined independently.
Then we need to perform size inference on each declaration of $\Gamma_G$ in order.
This is given by Rules \refnorule{a-global-nil}, \refnorule{a-global-assum}, and \refnorule{a-global-def} in \autoref{fig:algorithm-wf}.

In \refrule{a-global-def}, the \solve metafunction takes the constraint set $C_2$ and produces a size substitution that, when applied to the annotated term and type, ensures that they form a well-typed judgement. In otherwords, we are guaranteed that $\Sigma, \Gamma_G, \mt \vdash \rho e : \rho t$ holds.

Assuming for now that the corresponding constraint graph has no negative cycles, for every connected component, we want to assign each size variable the same common size variable but with a different size.
For instance, given the constraint set $\set{\upsilon_1 \sqsubseteq \hat{\upsilon}_2, \hat{\upsilon_1} \sqsubseteq \upsilon_3}$, a possible solution would be the mapping $\set{\upsilon_1 \mapsto \tau, \upsilon_2 \mapsto \upsilon^*, \upsilon_3 \mapsto \hat{\tau}}$.

This kind of problem is a \emph{difference constraint} problem \citep{clrs}.
Generally a solution involves finding a mapping from variables to integers, whereas our solution will map from size variables to size expressions with the same base, but the technique using a single-source shortest-path algorithm still applies.
Given a connected component $C_c$ with no negative cycles, our algorithm $\solvecomp$ for finding a solution proceeds as follows:

\begin{enumerate}
  \item Generate a fresh size variable $\tau$.
  \item For every size variable $\upsilon_i$ in $C_c$, add an edge $\tau \sqsubseteq \upsilon_i$ of weight $0$.
  \item \label{alg:solvecomp:shortest-path} Find the weights $w_i$ of the shortest paths from $\tau$ to every other size variable $\upsilon_i$ in $C_c$.
    This yields the constraint $\tau \sqsubseteq \hat{\upsilon}_i^{w_i}$.
  \item Na\"ively, we would map each $\upsilon_i$ to the size expression $\hat{\tau}^{-w_i}$.
    However, $-w_i$ may be negative, which would make no sense as the size of a size variable.
    Therefore, we find the largest weight $w_{\max} \coloneqq \max_i w_i$, and shift all the sizes up by $w_{\max}$.
    In other words, we return the map $\rho \coloneqq \set{\upsilon_i \mapsto \hat{\tau}^{w_{\max} - w_i}}$.
\end{enumerate}

Again, the time complexity of a single pass is $O(\norm{V}\norm{C_c})$ (where $V$ is the set of size variables in $C_c$)
due to
finding the single-source shortest paths in (\ref{alg:solvecomp:shortest-path}) using,
for instance, the Bellman--Ford algorithm.
The total $\solve$ algorithm, given some constraint graph $C$, is then as follows:

% Is removing negative cycles necessary? Could we prove that there are *no* negative cycles?
\begin{enumerate}
  \item Initialize an empty size substitution $\rho$.
  \item Find all negative cycles in $C$, and let $V^-$ be the set of all size variables in some negative cycle.
  \item Remove all edges with size variables in $V^-$ from $C$, and for every $\upsilon_i \in V^-$, add $\upsilon_i \mapsto \infty$ to $\rho$.
  \item For every connected component $C_c$ of $C$, add the mappings $\solvecomp{C_c}$ to $\rho$.
\end{enumerate}

Since dividing the constraint graph into connected components partitions the size variables and constraints into disjoint sets,
the time complexity of all executions of \solvecomp is $O(\norm{V}\norm{C})$.
This is also the time complexity of negative-cycle finding.
These two dominate the time complexity of finding the connected components, $O(\norm{V} + \norm{C})$.

The resulting size substitution $\rho$ will then satisfy $\rho \vDash C$.
As we will see in the following subsection,
with soundness this implies that $\Sigma, \Gamma_G, \mt \vdash \rho e : \rho t$ holds.

\subsection{Metatheory}\label{subsec:algorithm:metatheory}

%%% Local Variables:
%%% TeX-master: "../main.tex"
%%% TeX-engine: default
%%% End:
